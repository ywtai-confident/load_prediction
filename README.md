# load_prediction

## xgboost
    XGBoost全称是eXtreme Gradient Boosting，是一种梯度增强形式的集成学习，与bagging集成学习的方式不同，bagging的方式是通过对样本或特征进行选择抽样，从而组合成不同的训练集来进行训练，可以得到若干存在差异的模型，最后在预测时会将输入送给每个模型，将各个模型的输出进行组合汇总，得到一个更为准确的预测结果。Gradient Boosting的方式虽然同样会产生若干个模型，但模型需要按照一定的顺序进行预测，除了第一个模型拟合训练集的标签外，后面每个模型的目标是拟合上一个模型的残差，最终将所有模型的输出加在一起就可以得到一个更加准确的预测结果。大体流程如下：
真实标签：y
模型1预测目标：y，预测结果：y'
模型2预测目标：y-y' 预测结果：y''
模型3预测目标：y-y'-y'' 预测结果：y'''
最终模型预测输出：y'+y''+y'''≈y'+y''+y-y'-y''≈y
    在模型的选择上，通常会选择决策树(Decision Tree)作为基模型，因为决策树通过超参数的配置更容易产生差异较大的模型，从而增强模型的鲁棒性。
    Gradient Boosting是一种机器学习算法，而XGBoost是Gradient Boosting的一种工程化实现。相比传统的GBDT(Gradient Boosting Decision Tree)，XGBoost在以下几个方面做出了改善：
（1）以树模型作为基准模型时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。
（2）GBDT在模型训练时只使用了损失函数的一阶导数信息，XGBoost对损失函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
（3）传统的GBDT通常采用决策树这种CART（Classification And Regression Tree）作为基分类|回归器，XGBoost支持多种类型的基模型，比如线性分类|回归器。
（4）传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。
（5）传统GBDT没有设计对缺失值进行处理，XGBoost能够通过稀疏感知算法自动学习出缺失值的处理策略。
（6）传统GBDT没有进行并行化设计，注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。